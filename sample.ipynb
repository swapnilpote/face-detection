{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2, os, math, json\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers.core import Dense, Flatten, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path = 'data/'\n",
    "path = 'data/sample/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face = len([cls_1 for cls_1 in os.listdir(path + 'train/face/') if os.path.isfile(path + 'train/face/' + cls_1)])\n",
    "no_face = len([cls_2 for cls_2 in os.listdir(path + 'train/no_face/') if os.path.isfile(path + 'train/no_face/' + cls_2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28204, 780)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face, no_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create train, valid and sample folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.mkdir(path + 'valid')\n",
    "os.mkdir(path + 'sample')\n",
    "os.mkdir(path + 'sample/train')\n",
    "os.mkdir(path + 'sample/valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sub folder for categories\n",
    "os.makedirs(path + 'valid/face')\n",
    "os.makedirs(path + 'valid/no_face')\n",
    "os.makedirs(path + 'sample/train/face')\n",
    "os.makedirs(path + 'sample/train/no_face')\n",
    "os.makedirs(path + 'sample/valid/face')\n",
    "os.makedirs(path + 'sample/valid/no_face')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Move images from train/face/* folder into valid/face/*\n",
    "g = glob(path + 'train/face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 5)): shutil.move(shuf[i], path + 'valid/face/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move images from train/no_face/* folder into valid/no_face/*\n",
    "g = glob(path + 'train/no_face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 5)): shutil.move(shuf[i], path + 'valid/no_face/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move images from train/face/* folder into sample/train/face/*\n",
    "g = glob(path + 'train/face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 10)): shutil.copy2(shuf[i], path + 'sample/train/face/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move images from train/no_face/* folder into sample/train/no_face/*\n",
    "g = glob(path + 'train/no_face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 10)): shutil.copy2(shuf[i], path + 'sample/train/no_face/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move images from valid/no_face/* folder into sample/valid/no_face/*\n",
    "g = glob(path + 'valid/no_face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 10)): shutil.copy2(shuf[i], path + 'sample/valid/no_face/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move images from valid/face/* folder into sample/valid/face/*\n",
    "g = glob(path + 'valid/face/' + '*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "\n",
    "for i in range(int(len(g) / 10)): shutil.copy2(shuf[i], path + 'sample/valid/face/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(path, gen=ImageDataGenerator(), shuffle=True, batch_size=4, class_mode='categorical', \n",
    "                target_size=(224,224)):\n",
    "    return gen.flow_from_directory(path, target_size=target_size,\n",
    "                                   class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def onehot(x):\n",
    "    return to_categorical(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(path, target_size=(224,224)):\n",
    "    batches = get_batches(path, shuffle=False, batch_size=1, class_mode=None, target_size=target_size)\n",
    "    return np.concatenate([batches.next() for i in range(batches.n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classes(path):\n",
    "    batches = get_batches(path + 'train', shuffle=False, batch_size=1)\n",
    "    val_batches = get_batches(path + 'valid', shuffle=False, batch_size=1)\n",
    "    #test_batches = get_batches(path + 'test', shuffle=False, batch_size=1)\n",
    "    return (val_batches.classes, batches.classes, onehot(val_batches.classes), onehot(batches.classes),\n",
    "        val_batches.filenames, batches.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_at(model, layer_type):\n",
    "    layers = model.layers\n",
    "    layer_idx = [index for index,layer in enumerate(layers)\n",
    "                 if type(layer) is layer_type][-1]\n",
    "    return layers[:layer_idx+1], layers[layer_idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_array(fname, arr):\n",
    "    c=bcolz.carray(arr, rootdir=fname, mode='w')\n",
    "    c.flush()\n",
    "\n",
    "\n",
    "def load_array(fname):\n",
    "    return bcolz.open(fname)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 292 images belonging to 2 classes.\n",
      "Found 67 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = get_batches(path+'valid', batch_size=batch_size*2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 292 images belonging to 2 classes.\n",
      "Found 67 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_filenames = [f.split('/')[-1] for f in filenames]\n",
    "raw_val_filenames = [f.split('/')[-1] for f in val_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in vgg.model.layers: layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.add(Dense(batches.num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_2 (Lambda)            (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 8194      \n",
      "=================================================================\n",
      "Total params: 134,268,738\n",
      "Trainable params: 8,194\n",
      "Non-trainable params: 134,260,544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "292/292 [==============================] - 156s 535ms/step - loss: 0.0247 - acc: 0.9933 - val_loss: 0.0280 - val_acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2220ea5a58>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.fit_generator(batches, steps_per_epoch=batches.n, epochs=1, validation_data=val_batches, \n",
    "                        validation_steps=val_batches.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 292 images belonging to 2 classes.\n",
      "Found 67 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "trn = get_data(path + 'train')\n",
    "val = get_data(path + 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/trn.dat', trn)\n",
    "save_array(path+'results/val.dat', val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = load_array(path+'results/trn.dat')\n",
    "val = load_array(path+'results/val.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.model.compile(optimizer=Adam(1e-3),\n",
    "       loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swapnil/anaconda3/lib/python3.5/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292 samples, validate on 67 samples\n",
      "Epoch 1/3\n",
      "292/292 [==============================] - 5s 16ms/step - loss: 2.6745e-04 - acc: 1.0000 - val_loss: 0.0366 - val_acc: 0.9851\n",
      "Epoch 2/3\n",
      "292/292 [==============================] - 3s 11ms/step - loss: 9.7801e-06 - acc: 1.0000 - val_loss: 0.0100 - val_acc: 1.0000\n",
      "Epoch 3/3\n",
      "292/292 [==============================] - 3s 11ms/step - loss: 1.3479e-05 - acc: 1.0000 - val_loss: 0.0508 - val_acc: 0.9851\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f22206d3a58>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg.model.fit(trn, trn_labels, batch_size=batch_size, nb_epoch=3, validation_data=(val, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.load_weights(path+'results/ft1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_at(model, layer_type):\n",
    "    layers = model.layers\n",
    "    layer_idx = [index for index,layer in enumerate(layers)\n",
    "                 if type(layer) is layer_type][-1]\n",
    "    return layers[:layer_idx+1], layers[layer_idx+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_layers,fc_layers = split_at(vgg.model, Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.core.Lambda at 0x7f22205aa4e0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22205b35f8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22205de630>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f22204d3978>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22204cc160>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22204cc320>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f22204e80f0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22204df940>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f22204d4cf8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f222046fe48>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f2220575ba8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220fda668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220fda320>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220ffdda0>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7f2220f8f400>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220fb2a20>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220fb2c50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7f2220f52940>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.pooling.MaxPooling2D at 0x7f2220f75f60>,\n",
       " <keras.layers.core.Flatten at 0x7f2220f19470>,\n",
       " <keras.layers.core.Dense at 0x7f2220f19ba8>,\n",
       " <keras.layers.core.Dropout at 0x7f2220f3e278>,\n",
       " <keras.layers.core.Dense at 0x7f2220f3e3c8>,\n",
       " <keras.layers.core.Dropout at 0x7f2220ee7f28>,\n",
       " <keras.layers.core.Dense at 0x7f22205b9550>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = conv_model.predict(trn)\n",
    "conv_val_feat = conv_model.predict(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(path+'results/conv_val_feat.dat', conv_val_feat)\n",
    "save_array(path+'results/conv_feat.dat', conv_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = load_array(path+'results/conv_feat.dat')\n",
    "conv_val_feat = load_array(path+'results/conv_val_feat.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 14, 14, 512)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_val_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Dropout(p/4),\n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(p),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(p/2),\n",
    "        Dense(2, activation='softmax')\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 292 samples, validate on 67 samples\n",
      "Epoch 1/3\n",
      "292/292 [==============================] - 1s 3ms/step - loss: 1.7868 - acc: 0.8801 - val_loss: 1.6840 - val_acc: 0.8955\n",
      "Epoch 2/3\n",
      "292/292 [==============================] - 0s 938us/step - loss: 1.7112 - acc: 0.8938 - val_loss: 1.6840 - val_acc: 0.8955\n",
      "Epoch 3/3\n",
      "292/292 [==============================] - 0s 927us/step - loss: 1.7112 - acc: 0.8938 - val_loss: 1.6840 - val_acc: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f222054c550>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat, trn_labels, batch_size=batch_size, epochs=3, \n",
    "             validation_data=(conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
